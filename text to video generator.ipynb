{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt5--pN-KZJo",
        "outputId": "32f92546-f5cc-4ac5-9f95-9bef284d68b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q diffusers transformers accelerate torch\n",
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import CogVideoXPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "\n",
        "# 1. Load the 2B model in FP16 to save memory\n",
        "# CogVideoX-2B is the smaller, Colab-friendly version of the 5B model\n",
        "pipe = CogVideoXPipeline.from_pretrained(\n",
        "    \"THUDM/CogVideoX-2b\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 2. Critical Optimizations for Free Tier\n",
        "pipe.enable_model_cpu_offload()      # Moves parts of the model to CPU when not active\n",
        "pipe.enable_sequential_cpu_offload() # Extreme memory saving\n",
        "pipe.vae.enable_slicing()            # Decodes video frames one by one to prevent VRAM spikes\n",
        "pipe.vae.enable_tiling()             # Handles high-resolution tiles separately\n",
        "\n",
        "# 3. Generate Video\n",
        "prompt = \"A hyper-realistic cinematic shot of a futuristic city at night, heavy rain, neon lights reflecting on puddles, 4k, highly detailed.\"\n",
        "\n",
        "video_frames = pipe(\n",
        "    prompt=prompt,\n",
        "    num_videos_per_prompt=1,\n",
        "    num_inference_steps=50, # 50 is standard for quality; 30 for speed\n",
        "    num_frames=49,          # CogVideoX uses (8n + 1) frames. 49 frames = ~6 seconds\n",
        "    guidance_scale=6.0,\n",
        "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
        ").frames[0]\n",
        "\n",
        "# 4. Export as MP4\n",
        "export_to_video(video_frames, \"output_video.mp4\", fps=8)"
      ],
      "metadata": {
        "id": "353tlr57Kefj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"An aerial cinematic wide shot looking down through thick clouds at a sprawling mega-city. Massive skyscrapers pierce through the fog, glowing with golden amber lights. Flying vehicles leave long light trails like glowing veins between the towers. Moody, dark atmosphere, photorealistic, intricate textures, IMAX film aesthetic..\"\n",
        "\n",
        "video_frames = pipe(\n",
        "    prompt=prompt,\n",
        "    num_videos_per_prompt=1,\n",
        "    num_inference_steps=50, # 50 is standard for quality; 30 for speed\n",
        "    num_frames=49,          # CogVideoX uses (8n + 1) frames. 49 frames = ~6 seconds\n",
        "    guidance_scale=6.0,\n",
        "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
        ").frames[0]\n",
        "\n",
        "# 4. Export as MP4\n",
        "export_to_video(video_frames, \"output_video.mp4\", fps=8)"
      ],
      "metadata": {
        "id": "VNbj1OSVK93r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SByxLuSHQUGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}